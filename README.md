Question 1
-------

mapper and reducer in wc_{mapper,reducer}.py, run on GCE w/ hadoop. Reducer output in wc_feature.txt.

* 20 additional features (POS features generated by tagger from NLTK):
	1. length of body
	2. number of unique words
	3. Average number of words per sentence (split by capitalized words)
	4. number of capitalized words
	5. number of month words
	6. number of numbers
	7. ratio of numbers to words
	8. number of foreign words
	9. average word length
	10. number of nouns
	11. ratio of nouns to all words
	12. ratio of proper nouns to regular nouns
	13. number of conjunctions
	14. ratio of conjunctions to all words
	15. length of words without symbols, or determiners
	16. number of verbs
	17. ratio of verbs to all words
	18. ratio of past tense verbs to present tense verbs
	19. number of adjectives
	20. number of list item markers
* Features are calculated in features.py, written to extra_features_kaggle.json
	 

Question 2
------

*To reproduce*

	$ pip install -r requirements.txt
	$ python nyt.py --scrape --all -n 2000
	$ ipython
	[1] from nb import NYTClassifier
	[2] n = NYTClassifier('stopword.txt', 'nyt_unique.tsv')
	[3] n.train()
	[4] n.evaluate()

*Results*:

	F1 Score: 0.8475
	Time to train: 20.2s

*Most difficult to classify*

I used the classfier to predict the log probabilities of each class for each document, and took the 10 with the lowest pairwise probability difference for being in their respective classes.

*Discussion*

I used the sklearn implementation of bernoulli naive bayes, and could not introspect or graph parameters. However, given that the only features we are considering are word counts, 